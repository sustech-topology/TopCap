{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os # read system path \n",
    "import csv\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib qt5\n",
    "\n",
    "# Path is where the voiced/voicedless wav file located\n",
    "voicedPath=\"..\"\n",
    "voicedlessPath=\"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26437cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in os.listdir(voicedPath):\n",
    "    # Subsample dataset, retrieve 1 in 10 among dataset\n",
    "    randNum=np.random.randint(10)\n",
    "    if randNum !=0:\n",
    "        continue\n",
    "\n",
    "    # Read wav file as \"sig\"\n",
    "    fileName,ext=os.path.splitext(fn)\n",
    "    wavFile=voicedPath+fileName+\".wav\"\n",
    "    sig,samplerate=sf.read(wavFile)\n",
    "    \n",
    "    # Write result in a csv file\n",
    "    with open(\"MFCC_Diag3.csv\",\"a\",newline=\"\") as csvfile:\n",
    "        writer=csv.writer(csvfile)\n",
    "\n",
    "        # MFCC\n",
    "        mfccs = librosa.feature.mfcc(y=sig, sr=samplerate, n_mfcc=10, n_fft=int(len(sig)/2),hop_length=int(len(sig)/4))\n",
    "        mfccs=np.abs(mfccs.flatten())\n",
    "\n",
    "        # Add last feature to indicate if it is voiced/ voicedless\n",
    "        # 0 indicate the phone is voiced\n",
    "        data=np.append(mfccs,0)\n",
    " \n",
    "        writer.writerow(data)\n",
    "\n",
    "# For voicedless data\n",
    "for fn in os.listdir(voicedlessPath):\n",
    "    # Subsample dataset, retrieve 1 in 10 among dataset\n",
    "    randNum=np.random.randint(10)\n",
    "    if randNum !=0:\n",
    "        continue\n",
    "    \n",
    "    # Read wav file as \"sig\"\n",
    "    fileName,ext=os.path.splitext(fn)\n",
    "    wavFile=voicedlessPath+fileName+\".wav\"\n",
    "    sig,samplerate=sf.read(wavFile)\n",
    "\n",
    "    # Write result in a csv file\n",
    "    with open(\"MFCC_Diag3.csv\",\"a\",newline=\"\") as csvfile:\n",
    "        writer=csv.writer(csvfile)\n",
    "\n",
    "        # MFCC\n",
    "        mfccs = librosa.feature.mfcc(y=sig, sr=samplerate, n_mfcc=10, n_fft=int(len(sig)/2),hop_length=int(len(sig)/4))\n",
    "        mfccs=np.abs(mfccs.flatten())\n",
    "\n",
    "        # Add last feature to indicate if it is voiced/ voicedless\n",
    "        # 1 indicate the phone is voicedless\n",
    "        data=np.append(mfccs,1)\n",
    " \n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into DataFrame\n",
    "df=pd.read_csv('MFCC_Diag3.csv',header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle 9000 samples for each voice and voiceless type\n",
    "n=50\n",
    "# Shuffle 9000 rows from each class\n",
    "df_0 = df[df[n] == 0].sample(n=9000, random_state=42)\n",
    "df_1 = df[df[n] == 1].sample(n=9000, random_state=42)\n",
    "\n",
    "# Combine and shuffle again (optional)\n",
    "df = pd.concat([df_0, df_1]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP to reduce the feature dimension to 2\n",
    "reducer = umap.UMAP()\n",
    "data = df.iloc[:,0:n]\n",
    "scaled_data = StandardScaler().fit_transform(data)\n",
    "embedding = reducer.fit_transform(scaled_data)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d57bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the embedded feature as DataFrame\n",
    "df_feature=pd.DataFrame(embedding,columns=['feature1','feature2'])\n",
    "df_feature['type']=df[n]\n",
    "\n",
    "# Normalization: min-max\n",
    "normalized_df=(df_feature[['feature1','feature2']]-df_feature[['feature1','feature2']].min())/(df_feature[['feature1','feature2']].max()-df_feature[['feature1','feature2']].min())\n",
    "normalized_df['type']=df_feature['type']\n",
    "\n",
    "# Read/load data if needed\n",
    "#normalized_df.to_pickle('mfcc_df_feature_nor')\n",
    "#normalized_df=pd.read_pickle('mfcc_df_feature_nor')\n",
    "\n",
    "# Check the number of samples in each class\n",
    "(normalized_df['type']==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized result\n",
    "\n",
    "# Set up plot configuration\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "\n",
    "# Group the data based on voiced/ voicedless\n",
    "groups = normalized_df.groupby('type')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.margins(0.05)\n",
    "typeDict= {1:'voiceless',0:'voiced'}\n",
    "for type, group in groups:\n",
    "    if type==1:\n",
    "        ax.plot(group.feature1, group.feature2, marker='o', linestyle='', ms=2, label=typeDict[type],alpha=0.5, color='#4d4dff')\n",
    "    if type==0:\n",
    "        ax.plot(group.feature1, group.feature2, marker='o', linestyle='', ms=2, label=typeDict[type],alpha=0.5, color='#ff5c33')\n",
    "legend=ax.legend(fontsize=15,markerscale=4,loc='upper right')\n",
    "plt.xlabel('UMAP_1',fontsize=15)\n",
    "plt.ylabel('UMAP_2',fontsize=15)\n",
    "\n",
    "## Save figure as pdf file\n",
    "#plt.savefig(\"..\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual \n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "for type, group in groups:\n",
    "    if type==0:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(group.feature1, group.feature2, marker='o', linestyle='', ms=2, label=typeDict[type], alpha=0.5, color='#ff5c33')\n",
    "        plt.legend(['voiced'],fontsize=10,markerscale=4,loc='upper right')\n",
    "        plt.xlabel('UMAP_1')\n",
    "        plt.ylabel('UMAP_2')\n",
    "        plt.xlim([-0.1,1.1])\n",
    "        plt.ylim([-0.1,1.1])\n",
    "        plt\n",
    "    if type==1:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(group.feature1, group.feature2, marker='o', linestyle='', ms=2, label=typeDict[type], alpha=0.5, color='#4d4dff')\n",
    "        plt.legend(['voiceless'],fontsize=10,markerscale=4,loc='upper right')\n",
    "        plt.xlabel('UMAP_1')\n",
    "        plt.ylabel('UMAP_2')\n",
    "        plt.xlim([-0.1,1.1])\n",
    "        plt.ylim([-0.1,1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Save figure as pdf file\n",
    "#plt.savefig(\"..\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(normalized_df[['feature1','feature2']], normalized_df['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict class labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
